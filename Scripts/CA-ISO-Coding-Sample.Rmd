---
title: "CA Electric Demand"
author: "Noah Kouchekinia"
date: "2/27/2021"
output: html_document
---
# Introduction
This workbook scrapes, cleans, and analyzes electric demand data from the CA-ISO. Electrical consumption has recently received attention as an high frequency alternative measure of macroeconomic activity. For example, nation-wide energy consumption is one of several factors in the Federal Reserve Bank of New York's Weekly Economic Index.  The goal of this analysis is to evaluate electrical consumption as proxy for output, particularly in the context of the pandemic induced recession. 
```{r background_setup, include=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Documents/Job Search/Code Sample/CAISO/")
setwd("~/Documents/Job Search/Code Sample/CAISO/")
```

# Set Up
### Set Up Environment
```{r setup, message = FALSE, warning = FALSE}
#Set working directory
  setwd("/home/noah/Documents/Job Search/Code Sample/CAISO")

#Load Necessary Packages
  library(tidyverse)
  library(lubridate)
  library(ggplot2)
```

### Define Useful Functions
```{r function, message = FALSE, warning = FALSE}
#A function to take the difference between UK and CA time on any given day.
#Later, a API call will require times in GMT+-0, rather than local time. 
#The offset is not a constant of 8, because of different daylight saving schedules.
  tz_offset <- function(date,tz_a = "America/Los_Angeles",tz_b = "Europe/London"){
     a_time <- date %>%
                as.character() %>%
                paste("12:00:00")%>%
                as.POSIXct(tz_a)%>%
                with_tz(tz = tz_b)%>%
                hour()
    b_time <- date %>%
                as.character() %>%
                paste("12:00:00")%>%
                as.POSIXct(tz_a)%>%
                hour()
    offset <- a_time-b_time
    return(offset)
  }
```

### Aestetic Set-Up
```{r aestetic}
theme_nk <- theme_minimal
```

# Data Collection

### Scrape Latest CA-ISO Data

Here, I query from the California Independent System Operator's (ISO). Their OASIS API allowed me to pull hour by hour demand. However, querying from this API is challenging. Documentation is limited. Furthermore, the API limits the amount of data that can be downloaded with each pull, requiring many repetitive pulls. 

```{r scrape}
#Set query parameters
  start_date     <- as.Date("2009-01-01")
  end_date       <- Sys.Date()
  market_process <- "ACTUAL"              #Select from: 2DA, 7DA, DAM, ACTUAL, RTM

#Check for data downloaded in previous scrapes, adjust scraping start date accordingly
  extant_data <- read.csv("~/Documents/Job Search/Code Sample/CAISO/Data/Actual Hourly Integrated Load by Region.csv") #CHANGE THIS TO RELATIVE PATH BEFORE PUB
  extant_data$X <- NULL
  extant_data$OPR_DT <- as.Date(extant_data$OPR_DT)
  start_date <- max(start_date, max(extant_data$OPR_DT-1))
    
#Prepare dates used to loop through portions of days (30 at a time)
  #Create Vector of start and end dates
    all_dates <- as.Date(start_date:end_date,origin="1970-01-01") #all dates desired
    start_dates <- c(start_date, all_dates[1:length(all_dates)%%30==0]) #start dates every 30 days
    end_dates   <- unique(c(start_dates[2:length(start_dates)],end_date)) #offset start dates to get end dates
    if(length(start_dates)==1){end_dates <- end_date} #needed in case of only one scraping period
  #Get correct dates to include tz offset 
    start_offset <- tz_offset(start_dates)
    end_offset <- tz_offset(end_dates)
  #Create list the length of start_dates, in order to hold query results  
    data_list <- vector("list",length(start_dates))

#Set other query parameters
  q_base <- "http://oasis.caiso.com/oasisapi/SingleZip?queryname=SLD_FCST"
  q_process <-  paste0("market_run_id=",market_process)
  q_format <-  "resultformat=6"
  q_v <- "version=1"

#Pull one thirty day period at a time
  for(i in 1:length(start_dates)){
    #Set variable query components
      q_start_date <-  paste0("startdatetime=",format(start_dates[i], "%Y%m%d"),"T0",start_offset[i],":00-0000")
      q_end_date <-  paste0("enddatetime=",format(end_dates[i], "%Y%m%d"),"T0", end_offset[i],":00-0000")
      query <-paste(q_base, q_process, q_format, q_start_date, q_end_date, q_v, sep="&") #Query URL for the OASIS API
    #Assemble query
      query <-paste(q_base, q_process, q_format, q_start_date, q_end_date, q_v, sep="&") #Query URL for the OASIS API
    #Download Query
      tf = tempfile() #create temporary destination file
      download.file(query,tf) #download file to tempfile
      fname = unzip(tf,list=T)$Name[1] #Unzip downloaded file
      data_list[[i]] <- read.csv(unz(tf, fname)) #Read delimited data from file
      Sys.sleep(5) #Pause, so as not to sent overlapping queries
    }

#Clean and export choice cuts of data
  caiso_df <- data_list[unlist(lapply(data_list,ncol))==14] %>%  
    bind_rows()%>%
    select(c("OPR_DT","OPR_HR","TAC_AREA_NAME","LABEL","POS","MW"))%>%
    mutate(OPR_DT = as.Date(OPR_DT),
           TAC_AREA_NAME <- as.character(TAC_AREA_NAME))%>%
    bind_rows(extant_data)%>%
    group_by(OPR_DT,OPR_HR,TAC_AREA_NAME) %>%
    summarize(LABEL = last(LABEL), POS = last(POS),MW = last(MW))
    
#Conclude
  write.csv(caiso_df, "./Data/Actual Hourly Integrated Load by Region.csv",row.names = FALSE) #Save Data
  keep_list <- c("caiso_data", "start_date", "end_date")
  rm(list = ls()[!ls() %in% keep_list]) #Tidy Up Environment
```

## Pull Degree Day Data

As I will soon demonstrate, electrical demand data is extremely volatile, and depends on things unrelated to economic output. Perhaps the largest contributor is weather; HVAC systems use a lot of energy. The NOAA produces releases "degree day" data, which are a measure of how much the temperature in a region deviates from room temperature, weighted by population. These degree day series are used by utilities to model and predict electrical consumption. We will use them to correct our electrical output series for weather. 

```{r pressure, echo=FALSE}
#Set Query Parameters
  url_start <- "https://ftp.cpc.ncep.noaa.gov/htdocs/degree_days/weighted/daily_data/"
  years <- (year(Sys.Date())-10):year(Sys.Date())
  categories <-c("/StatesCONUS.Cooling","/StatesCONUS.Heating")
  extension <- ".txt"
  
#Create list to hold results
  heat_tables <- cool_tables <-vector("list", length=length(years))
  
#Pull data year by year and category by categry (heat and cool degree days) with loops
#Double looping is not great, but iterations are low here.
  i <- 1
  for(year in years){
    for(category in categories){
      api_call <- paste0(url_start,year,category,extension)
      table <- read.table(api_call, sep = "|", skip = 3, stringsAsFactors = FALSE)
      table <- t(table)
      colnames(table) <- table[1,]
      colnames(table)[1] <- "date"
      table <- table[-1,]
      table <- as.data.frame(table, stringsAsFactors = FALSE)
      if(grepl("Heat",category)) heat_tables[[i]] <- table
      if(grepl("Cool",category)) cool_tables[[i]] <- table
    }
    i = i+1
  }
  
#Combine and Clean results
  heat_data <- bind_rows(heat_tables)%>%
    pivot_longer(-date)%>%
    mutate(type = "heat")
  cool_data <- bind_rows(cool_tables)%>%
    pivot_longer(-date)%>%
    mutate(type = "cool")
  degree_days <- bind_rows(heat_data, cool_data)%>%
    mutate(date <- as.Date(date,format = "%Y%m%d"),
           value <- as.numeric(degree_days$value))
  names(degree_days)[2]<- "state"
  
#Conclude 
  write.csv(degree_days, "./Data/degree_days.csv", row.names = FALSE)
  keeplist <- c(keep_list, "degree_days")
  rm(list = ls()[!ls() %in% keep_list]) #Tidy Up Environment
```

## Create Weekday/Holiday Series

```{r holiday}
x <- "holiday"
```

#
